# compose-ci

## TL;DR

``` bash
# docker-machine create --driver <driver> my_ci
eval (docker-machine env my_ci)
docker run -d \
    -e HOOK="docker-compose run tests" \
    -e GITHUB_TOKEN -e GITHUB_REPO \
    -v /var/run/docker.sock:/var/run/docker.sock \
    -p 8080:80 \
    docteurklein/compose-ci

curl -X POST \
    -H "Authorization: token ${GITHUB_TOKEN}" \
    "https://api.github.com/repos/${GITHUB_REPO}/hooks" \
    -d@- <<JSON
      { "name": "web", "active": true, "events": [ "push" ], "config": {
        "url": "http://$(docker-machine ip my_ci):8080/?token=${GITHUB_TOKEN}",
        "content_type": "json",
        "insecure_ssl": "1"
      }}
JSON
```

## What ?

A simple, docker(-compose) enabled, alpine-based container, listening for github webhooks.

It will:
 - [notify](https://developer.github.com/v3/repos/statuses/) github of the build status
 - download and extract the corresponding tarball, then `cd` inside it
 - export some environment variables (`$COMPOSE_PROJECT_NAME`, `$commit`, `$short_commit`, â€¦)
 - execute whatever `$HOOK` command you configured
 - send a mail containing the `$HOOK` command output
 - cleanup running containers, networks and logs afterwards

## Why ?

SaaS CIs are cool and all, but you clearly don't have the same flexibility as this \o/  
This also prepares the environment with latest versions of docker and compose,  
making it a breeze to run your tests.

It's self hosted. It can be used wherever docker is running.

## How to customize ?

 > Note: This is optional.

You can extend the base image and provide a `server.pem` certificate.  
In order to use it, you can `COPY` it in the image, or mount it using:

    docker run \
        -v my-cert.pem:/certs/my-cert.pem \
        -e CERT_PATH=/certs/my-cert.pem

### Create the `server.pem` file

    openssl req -new -x509 -keyout server.pem -out server.pem -days 365 -nodes

> Note: the certificate must match the public IP (or hostname) that will be used for the webhook.

### Create a `Dockerfile`

``` Dockerfile
FROM docteurklein/compose-ci

COPY server.pem /certs/server.pem
COPY hook /hook # optional, to embed a hook file

# add extra packages
# RUN apk add --update <packages>
```

> Note: don't forget to add the `-e CERT_PATH=/certs/server.pem` env variable.

### Embed a `hook` file

> Note: this is optional. Run this with the appropriate `$HOOK` config.  
> You have access to the `docker`, `aws`, and `docker-compose` commands, plus all what alpine proposes.  
> All the output (stderr included) of this script will be sent to you by mail.  

``` bash
#!/bin/bash

set -exuo pipefail # stop on failure

# logging into aws ECR (docker login), for private registry
eval $(aws ecr get-login)

docker-compose pull

# build latest images
docker-compose build

# run containers
docker-compose up -d

docker-compose run tests
# or ./bin/ci
# or ./bin/test

# push latest images in ECR only if tests pass
images=( "my" "images" "names" )
for image in "${images[@]}"
do
    docker push ${DOCKER_REGISTRY}/${image}:latest # (latest or $short_commit)
done
```

> Note: Don't forget to `chmod +x hook`.

### Run it

In order to listen to github webhooks, you'll need to define some environment variables.

> Note: All these variables will also be passed to the `hook` command.

Some variables are mandatory:

 - `$HOOK` the command to execute
 - `$GITHUB_TOKEN` a valid o-auth token
 - `$GITHUB_REPO` to know which tarball to download

Some are optional:

 - `$SMTP_*` if you want to receive emails
 - `$CERT_PATH` the absolute path to your https certificate
 - `$GARBAGE_COLLECT` set to 0 to keep the build contexts on disk (default 1)
 - `$KEEP_LOGS` set to 0 to remove logs from disk (default 1)
 - whatever else that is required by your command

``` bash
docker build -t my_ci .

docker run -it --rm \
    -p 8080:80 \
    -e HOOK="/hook" \
    -e GITHUB_TOKEN -e GITHUB_REPO \
    -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY -e AWS_DEFAULT_REGION \
    -e SMTP_FROM -e SMTP_TO -e SMTP_HOST -e SMTP_PORT -e SMTP_USER -e SMTP_PASS \
    -v /var/run/docker.sock:/var/run/docker.sock \
    my_ci
```

> Note: We mount `docker.sock` inside the container.  
> /!\ This means that any docker command executed by the `hook` will be made against the **host** docker engine.


*Tadaaa!* You now have a service waiting for web hooks on port 8080.  
Grab the IP address of your host and configure a [github web hook](https://developer.github.com/webhooks/).

The url **MUST** contain [`?token=<my-oauth-token>`](https://github.com/settings/tokens/new).  
This token **MUST** match `$GITHUB_TOKEN`.

> Note: Depending wether your certificate (`server.pem`) is correctly signed, you might have to disable SSL verification.

## Manually trigger a build:

You can simulate a github webhook request using:

    curl -X POST "$(docker-machine ip my_ci):8080/?token=$GITHUB_TOKEN" -d '{"after": "<commit>"}'

## Volumes

This image declares 2 volumes:

### /tmp/logs

Stores all the execution outputs.
If you decided to keep the logs, you can retrieve them using the build uuid:

    less /tmp/logs/<uuid>/all

> Note: **all** the logs are also redirected to docker logs.

### /tmp/builds

Stores all extracted tarballs.
That's where is executed the command.
It's cleaned-up after each build, unless you deactivated garbage collecting.

If you decided to keep the builds, you can retrieve them using the build uuid:

    ls /tmp/builds/<uuid>/

> Note: The uuid is visible at the end of every email and in the response of the webhook http request.


## A note about security

It is SSL-ready, but not enabled by default. You need to generate a certificate and enable it,
by using the `CERT_PATH` variable.

The only verification that is made is the `$GITHUB_TOKEN` comparison...
That same token is used to authenticate against the github API.

This is **not** intended to replace a multi-tenant SaaS.  
This is **not** isolated. All the builds share the same context.  
This is however not really important since you can launch one instance per project.

If you mount the docker socket, **anyone** can do **anything** to your host.  
If you don't want that, take a look at [--userns-remap](https://docs.docker.com/engine/reference/commandline/daemon/#starting-the-daemon-with-user-namespaces-enabled).  
But even then, the same docker engine is shared for every build.


